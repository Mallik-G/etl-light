
// THIS IS AN EXAMPLE CONFIGURATION - CHANGE IT TO YOUR NEEDS

spark {
  app-name = "kafka-etl"
  config {
    spark.executor.memory = "1g"
    spark.eventLog.enabled = "true"
    spark.eventLog.dir = "hdfs:/user/spark/applicationHistory"
    spark.ui.port = "4045"
    spark.driver.extraJavaOptions = "-DSPARK_YARN_MODE=true"
    spark.task.maxFailures = "4"
  }
}

kafka {
  topics = ["events"]
  config {
    group.id = "kafka-etl"
    metadata.broker.list = "stress1-hadoop-dn-1.openstacklocal:9092"
    auto.offset.reset = "smallest"
  }
}

etl {

  lock {
    enabled = false
    zookeeper-connect = "stress1-hadoop-dn-1.openstacklocal:2181"
    path = "/etl/file.lock"
    wait-for-lock-seconds = 10
  }

  state {
    folder = "hdfs:///data/etl/work/state"
    files-to-keep = 10
  }

  errors-folder = "hdfs:///data/etl/errors/events"

  pipeline {

    factory-class = "yamrcraft.etlight.pipeline.ParquetPipelineFactory"

    transformer = {
      config = {
        timestamp-field = "ts"
        timestamp-field-format = "yyyy-MM-dd HH:mm:ss"

        default-schema-file = "hdfs:///user/spark/etl/audit-event.avsc"

        schema-selection {
          field = "resource_type"
          schemas {
            availability = "hdfs:///user/spark/etl/availability.avsc"
            GooglePlaces = "hdfs:///user/spark/etl/GooglePlaces.avsc"
          }
        }
      }
    }

    writer = {
      config = {
        working-folder = "hdfs:///data/etl/work/.staging/events"
        output-folder = "hdfs:///data/etl/output/events"
        partition {
          pattern = "YYYY/MM/dd/HH"
        }
        record-name-to-folder-mapping {
          AuditEvent = "audit_events"
          Availability = "availability_events"
          GooglePlaces = "places_events"
        }
      }
    }
  }

}